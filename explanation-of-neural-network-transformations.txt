> Within neural networks, linear transformations play a crucial role in mapping high dimensional data to low dimensional representations. These transformations are generally *lossy* in nature since the data cannot be *exactly* reconstructed from its low dimensional representation. In statistical inference, the aim is essentially to *losslessly compress* a dataset $D$ as much as possible. Then for any fixed model class $\mathcal{M}$, ideally we want $M \in \mathcal{M}$ to retain as much information about $D$ as possible, i.e. where $I(M ; D)$ is large, such that $L(D|M)$ (the residual error) is small. $M$ can do this in a few ways. If $M$ has enough space, it can more or less memorise the whole of $D$. However, if $M$ is small compared to $D$, then the only way it can achieve high $I(M ; D)$ is if it finds *higher-level* features in $D$. These features relate together (i.e. reduce uncertainity about) a *larger number* of specific observations in $D$, allowing for a more concise representation of the same data. For instance, it's more efficient to describe images of cats in terms of salient features such as "eyes" and "tails" rather than specific arrangements of edges. If we know the "eye" feature is present in the top-left corner of an image, then the distribution over pixels within that region is lower entropy, compared to if we had just been told that "an edge" exists in the top-right. A related notion from algorithmic information theory also says that such patterns are likely to be *genuine* in that they're present in the distribution $p$ from which $D$ was sampled. This is due to the no-hypercompression inequality and related theorems. 
>
> The effect of incentivising high $I(M ; D)$ results in learned linear transformations that map $d \in D$ to vectors encoding the presence of high-level features detected in $d$. 
