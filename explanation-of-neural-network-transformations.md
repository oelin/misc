Within neural networks, linear transformations play a crucial role in mapping high dimensional data to low dimensional representations. These transformations are generally *lossy* in nature since the data cannot be *exactly* reconstructed from its low dimensional representation. To understand these representations more deeply, we need to first understand how they're derived.

In statistical inference, the aim is essentially to *losslessly compress* a dataset $D$ as much as possible. Then for any fixed model class $\mathcal{M}$, ideally we want $M \in \mathcal{M}$ to retain as much information about $D$ as possible, i.e. where $I(M ; D)$ is large, such that $L(D|M)$ (the residual error) is small. $M$ can do this in a few ways. If $M$ has enough space, it can more or less memorise the whole of $D$. However, if $M$ is small compared to $D$, then the only way it can achieve high $I(M ; D)$ is if it can *generalise* specific observations in $D$. Instead of assuming each bit of $D$ is independent, generalisation involves *relating* bits together, i.e. assuming they share some non-negligible *mutual information*. This allows for a more efficient encoding since each bit can be described with reference to previously described bits.

When training neural networks, $D$ is typically a set of examples, with each example being a real vector of $k$ dimensions. The conventional assumption is that each example is i.d.d. and sampled from some multinomial distribution $p$. Then the only relationships $M$ can exploit are those found *within* examples. For instance, the relationships between adjacent pixels within images.

In neural networks, these relationships are encoded as *features*. A feature is general *pattern* of bits found within an example. For instance, the patterns `10101`, `11011`, `10001` are all examples of 5-bit palindromes. We might set up a "feature detector" for such patterns which outputs `1` if one is found within a string and `0` otherwise. This would be an example of a binary feature, since it's either present or *not* present. However, more common in neural networks are *continuous* features who's presence is gives as a real number along some interval. For instance, when using the sigmoid activation function, this number can be interpreted as the *probability* that the feature is present. Other activation functions like ReLU are also common (which we'll discuss later).

In any case, features *quantify* the extent to which a pattern appears within an example. The activation of a feature can be calculated by multiplying each component in the example vector, by a *learnable* weight and then summing up the result. The sum is then fed through the activation function. Bias weights are also added to the sum prior to activation. To perform this operation in parallel for $n$ feature detectors, we compute a linear transformation on the $k$-dimensional example using a weight matrix $W$. We then add the bias vector to the result and feed it through the activation function. Hence, $\sigma(Wx + b)$ where $W$ has shape $k \times n$ computes the activation of $n$ features given a $k$-dimensional example vector.

Which specific features are learned depends on $p$. In general, features that are *salient* (i.e. present within many examples) will allow for the greatest overall compression of the dataset. For example, if $p$ is a distribution over 256x256 cat images, then features such as "eye" and "tail" might be useful (both in representing the image itself, and in prediction tasks). Note that these can't be directly ascertained from a single linear transformation. We usually require some lower-level features such as "edge" and "circle" to be encoded first. The itermediate layers of deep neural networks support compositionality of learned features, so that increasingly abstract concepts can be represented. With sufficiently many layers, and enough data to provide examples of high-level features, this can lead to excellent compression of individual examples.
